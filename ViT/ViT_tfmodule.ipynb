{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def __call__(self,inputs):\n",
    "        # (batch, height, width, patchsize)\n",
    "        return tf.reshape(inputs,shape=[inputs.shape[0],-1, inputs.shape[-1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjection(tf.Module):\n",
    "    def __init__(self, projecition_dim, name=None):\n",
    "        super().__init__(name)\n",
    "        self.dense = tf.keras.layers.Dense(units=projecition_dim,name=name)\n",
    "\n",
    "    def __call__(self,inputs):\n",
    "        # (batch, patchnum, patchsize)\n",
    "        assert inputs.ndim==3, \"input shape to the LinearProjecition is wrong\"\n",
    "        x = self.dense(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionEmbs(tf.Module):\n",
    "    # add learable 1D position embeding \n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def __call__(self, inputs): \n",
    "        # (batch, patchnum+1, projection_dim)\n",
    "        # print(inputs.shape)\n",
    "        position_emb_shape = (1, inputs.shape[1], inputs.shape[2])\n",
    "        self.position_emb = tf.Variable(\n",
    "            initial_value=tf.random.normal(position_emb_shape),\n",
    "            name='PositionEmbs',\n",
    "            trainable=True\n",
    "        )\n",
    "        return inputs + self.position_emb # broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddDummy(tf.Module):\n",
    "    # attach learnable parameter to inputs\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # (batch, patchnum, projection_dim)\n",
    "\n",
    "        assert inputs.ndim == 3, \"input shape to the AddDummy is wrong\"\n",
    "        dummy_shape = (1, 1, inputs.shape[2])\n",
    "        self.dummy = tf.Variable(\n",
    "            initial_value=tf.random.normal(dummy_shape),\n",
    "            name = 'dummy_patch',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        dummy = tf.repeat(self.dummy,repeats=[inputs.shape[0]], axis=0)\n",
    "        y = tf.concat([dummy,inputs], axis=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(tf.Module):\n",
    "    # 1. add dummy patch and project patches to projection dimension \n",
    "    # 2. attach position embeddings to each patches \n",
    "    # NOTE add extra learnable class embedding to dummy patch\n",
    "\n",
    "    def __init__(self, projection_dim, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.flatten = Flatten(name=name+'_flattening')\n",
    "        self.projection = LinearProjection(projecition_dim=projection_dim, name=name+'_linear_projection')\n",
    "        self.add_dummy = AddDummy()\n",
    "        self.add_posemb = AddPositionEmbs()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # (batch, height, width, patchsize)\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.projection(x)\n",
    "        x = self.add_dummy(x)\n",
    "        x = self.add_posemb(x)\n",
    "\n",
    "        return x # (batch, patchnum+1, patchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2, 2), dtype=float32, numpy=\n",
       "array([[[ -0.42476523,   8.029542  ],\n",
       "        [ 41.232414  ,   6.809023  ]],\n",
       "\n",
       "       [[ 39.513515  ,  55.355705  ],\n",
       "        [ 48.67237   ,  44.746834  ]],\n",
       "\n",
       "       [[ 83.49648   ,  75.89939   ],\n",
       "        [101.47449   , 114.71094   ]],\n",
       "\n",
       "       [[119.910255  , 119.23215   ],\n",
       "        [121.887856  , 176.67961   ]],\n",
       "\n",
       "       [[124.84429   , 170.56232   ],\n",
       "        [187.56752   , 172.61804   ]]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.constant(np.arange(20).reshape(5, 2, 2) * 10, dtype=tf.float32)\n",
    "noise = tf.random.normal(shape=[5,2,2],stddev=20)\n",
    "data = noise+data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[-1.434181  , -1.4138632 ],\n",
       "       [-0.69455564, -0.7045014 ],\n",
       "       [ 0.02757359,  0.00374174],\n",
       "       [ 0.69525576,  0.6906787 ],\n",
       "       [ 1.4059069 ,  1.4239436 ]], dtype=float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = tf.keras.layers.LayerNormalization(axis=0)\n",
    "n1(data)[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.9999951 , -0.9999956 ],\n",
       "       [ 0.9999952 ,  0.99999535]], dtype=float32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "n2(data)[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.999972    0.999972  ]\n",
      " [-0.9999917   0.99999243]\n",
      " [ 0.9999648  -0.9999659 ]\n",
      " [ 0.99567866 -0.9956788 ]\n",
      " [-0.9999993   0.99999887]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.99999833 -0.99999815]\n",
      " [ 0.9998695  -0.99987096]\n",
      " [-0.99998856  0.99998873]\n",
      " [-0.9999994   0.9999992 ]\n",
      " [ 0.999992   -0.9999902 ]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "n3 = tf.keras.layers.LayerNormalization(axis=2)\n",
    "print(n3(data)[:,0,:])\n",
    "print(n3(data)[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.Module):\n",
    "    # MLP layer for transformer encoder\n",
    "    # two layers with a GELU non-linearity\n",
    "\n",
    "    # dense-gelu-dropout-dense-dropout\n",
    "    def __init__(self, mlp_dim, output_dim, dropout_rate, name):\n",
    "        super().__init__(name=name)\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dense_1 = tf.keras.layers.Dense(\n",
    "            mlp_dim, \n",
    "            kernel_initializer = tf.keras.initializers.GlorotNormal(),\n",
    "            bias_initializer = tf.keras.initializers.RandomNormal(stddev=1e-6), \n",
    "            activation=tf.nn.gelu, \n",
    "            trainable=True,\n",
    "            name=name+'_dense1'\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate, name=name+'_dropout')\n",
    "        self.dense_2 = tf.keras.layers.Dense(\n",
    "            output_dim, \n",
    "            kernel_initializer = tf.keras.initializers.GlorotNormal(),\n",
    "            bias_initializer = tf.keras.initializers.RandomNormal(stddev=1e-6), \n",
    "            activation=tf.nn.gelu, \n",
    "            trainable=True,\n",
    "            name=name+'_dense2'\n",
    "        )\n",
    "            # kernel은 xavier uniform/ bias는 normal(stddev=1e-6)\n",
    "            # TODO check activaiton GELU's location \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        y = self.dense_1(inputs)\n",
    "        y = self.dense_2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_DIM = 100\n",
    "DROPOUT_RATE = 0.7\n",
    "ATTENTION_DROPOUT_RATE = 0.9\n",
    "# ATTENTION_HIDDEN_DIM\n",
    "# ATTENTION_OUTPUT_DIM\n",
    "# TODO 매개변수 편하게 표현할 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.Module):\n",
    "    # Layer Norm / MHA+dropout / skip-connection / Layer Norm/ MLP / skip-connection\n",
    "\n",
    "    def __init__(self, projection_dim, name=None):\n",
    "        super().__init__(name)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(axis=0, name=name+'_layer_norm')\n",
    "        self.MHA = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=1,\n",
    "            key_dim=50,\n",
    "            value_dim=100,\n",
    "            dropout=ATTENTION_DROPOUT_RATE,\n",
    "            use_bias=False,\n",
    "            #output_shape=(-1,input_shape[1],self.hidden_dim),\n",
    "            attention_axes=(1,2),\n",
    "\n",
    "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "            trainable=True,\n",
    "            name=name+'_MHA'\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(DROPOUT_RATE,name=name+'_dropout')\n",
    "        self.MLP = MLP(\n",
    "            mlp_dim=MLP_DIM, \n",
    "            output_dim=projection_dim, \n",
    "            dropout_rate=DROPOUT_RATE, \n",
    "            name=name+'_encoder_mlp'\n",
    "        )\n",
    "        # TODO check the value of mlpdim and dropout_rate\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x1 = self.layer_norm(inputs)\n",
    "        x1 = self.MHA(x1,x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        skip2 = x1 + inputs\n",
    "        x2 = self.layer_norm(skip2)\n",
    "        x2 = self.MLP(x2)\n",
    "        outputs = x2+skip2\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.Module):\n",
    "\n",
    "    def __init__(self, block_num, projection_dim, name=None):\n",
    "        super().__init__(name)\n",
    "        self.block_num = block_num\n",
    "        self.patch_embedding = PatchEmbedding(projection_dim=projection_dim, name='patch_embs')\n",
    "        self.encoder_block = EncoderBlock(projection_dim=projection_dim, name='encoder_block')\n",
    "        self.encoder_norm = tf.keras.layers.LayerNormalization(axis=0, name='encoder_norm')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        self.emb = self.patch_embedding(inputs)\n",
    "        for i in range(self.block_num):\n",
    "            self.encoded = self.encoder_block(self.emb)\n",
    "\n",
    "        # self.encoded_norm = self.encoder_norm(self.encoded)\n",
    "        '''\n",
    "        TODO check encoder norm is needed.\n",
    "        encoded_norm makes the final result all zero.\n",
    "        '''\n",
    "        \n",
    "        return self.encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(tf.Module):\n",
    "    def __init__(self, patch_size,block_num, projection_dim, class_num, name=None):\n",
    "        super().__init__(name)\n",
    "        self.patch_size = patch_size\n",
    "        self.class_num = class_num\n",
    "        self.encoder = Encoder(block_num = block_num, projection_dim=projection_dim)\n",
    "        self.classifier = tf.keras.layers.Dense(class_num, name='classifier')\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # (batch, height, width)\n",
    "        patches = tf.image.extract_patches(\n",
    "            inputs,\n",
    "            sizes=[1,self.patch_size,self.patch_size,1], \n",
    "            strides=[1,self.patch_size,self.patch_size,1],\n",
    "            rates=[1,1,1,1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        if inputs.shape[0] == 1:\n",
    "            # batch가 아닌 1개의 이미지가 들어온 경우\n",
    "            # 배치 개수만큼 늘려주자\n",
    "            patches = tf.repeat(input=patches, repeats=[inputs.shape[0]], axis=0)\n",
    "            \n",
    "        encoded = self.encoder(patches)\n",
    "        self.class_token = encoded[:,0,:]\n",
    "        if len(self.class_token.shape) == 3:\n",
    "            self.class_token = tf.squeeze(self.class_token, axis=1)\n",
    "\n",
    "        outputs = self.classifier(self.class_token)\n",
    "        outputs = self.softmax(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    image_resized = tf.image.resize(image, (32, 32))\n",
    "    image_resized = tf.cast(image_resized, tf.float32)\n",
    "    image_resized = (image_resized - 127.5) / 127.5\n",
    "    return tf.expand_dims(image_resized, 0).numpy()\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image = preprocess_image(image)\n",
    "    return image\n",
    "\n",
    "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
    "image = load_image_from_url(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.patch_size = 16\n",
    "\n",
    "resized_image = tf.image.resize(image, [64,32])\n",
    "images = tf.repeat(image,[100], axis=0)\n",
    "patches = tf.image.extract_patches(images, \n",
    "                    sizes=[1,self.patch_size,self.patch_size,1], \n",
    "                    strides=[1,self.patch_size,self.patch_size,1],\n",
    "                    rates=[1,1,1,1],\n",
    "                    padding='VALID'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f11b03d2160>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(resized_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f11b045fe20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 2, 2, 768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 5, 100])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PatchEmbedding(projection_dim=100)\n",
    "p(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 2, 2, 768])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 5, 100])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Encoder(projection_dim=100, name='encoder')\n",
    "e(patches, block_num=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e.trainable_variables)\n",
    "# MHA - 4\n",
    "# MLP - 2*2\n",
    "# LM in MLP - 2\n",
    "# LM in encoder - 2\n",
    "# dummy - 1\n",
    "# posemb - 1\n",
    "# linear projection - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.stack([image]*10)\n",
    "images = tf.squeeze(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1000), dtype=float32, numpy=\n",
       "array([[0.00099019, 0.00132887, 0.00110782, ..., 0.00078618, 0.00091062,\n",
       "        0.00100394],\n",
       "       [0.00099019, 0.00132887, 0.00110782, ..., 0.00078618, 0.00091062,\n",
       "        0.00100394],\n",
       "       [0.00099019, 0.00132887, 0.00110782, ..., 0.00078618, 0.00091062,\n",
       "        0.00100394],\n",
       "       ...,\n",
       "       [0.00099019, 0.00132887, 0.00110782, ..., 0.00078618, 0.00091062,\n",
       "        0.00100394],\n",
       "       [0.00099019, 0.00132887, 0.00110782, ..., 0.00078618, 0.00091062,\n",
       "        0.00100394],\n",
       "       [0.00099019, 0.00132887, 0.00110782, ..., 0.00078618, 0.00091062,\n",
       "        0.00100394]], dtype=float32)>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = VisionTransformer(patch_size=16, block_num=1, projection_dim=20, class_num=1000)\n",
    "vit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(patch_size=16, block_num=1, projection_dim=20, class_num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_sample = tf.keras.utils.to_categorical(y_test[:10], 1000)\n",
    "y_train_sample = tf.keras.utils.to_categorical(y_test[:10], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "x_test_dataset = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "y_test_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "train_dataset = tf.data.Dataset.zip((x_train_dataset, y_train_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((x_test_dataset, y_test_dataset))\n",
    "\n",
    "train_batch = train_dataset.batch(100)\n",
    "test_batch = test_dataset.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO make training code faster\n",
    "# Given a callable model, inputs, outputs, and a learning rate...\n",
    "def train(model, x, y):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Trainable variables are automatically tracked by \n",
    "    y = tf.keras.utils.to_categorical(y,1000)\n",
    "    y_ = model(x)\n",
    "    current_loss = cce(y, y_)\n",
    "\n",
    "  # Use GradientTape to calculate the gradients with respect to parameters in model\n",
    "  gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  # Subtract the gradient scaled by the learning rate\n",
    "  # model.w.assign_sub(learning_rate * dw)\n",
    "  # model.b.assign_sub(learning_rate * db)\n",
    "  return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "# Define a training loop\n",
    "def training_loop(model, batch):\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # Update the model with the single giant batch\n",
    "    for step, (x_batch, y_batch) in enumerate(batch):\n",
    "      minibatch_loss = train(model, x_batch, y_batch)\n",
    "\n",
    "    # Track this before I update\n",
    "    # Ws.append(model.w.numpy())\n",
    "    # bs.append(model.b.numpy())\n",
    "\n",
    "    print(\"Epoch %2d: loss=%2.5f\" % (epoch, minibatch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 19:25:58.088776: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-11-15 19:25:58.122016: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: loss=6.91923\n",
      "Epoch  1: loss=6.91296\n",
      "Epoch  2: loss=7.00701\n",
      "Epoch  3: loss=6.94552\n",
      "Epoch  4: loss=6.90667\n",
      "Epoch  5: loss=6.95655\n",
      "Epoch  6: loss=6.91465\n",
      "Epoch  7: loss=6.91146\n",
      "Epoch  8: loss=6.81367\n",
      "Epoch  9: loss=6.78720\n",
      "Epoch 10: loss=6.70989\n",
      "Epoch 11: loss=6.74984\n",
      "Epoch 12: loss=6.64445\n",
      "Epoch 13: loss=6.82197\n",
      "Epoch 14: loss=6.61170\n",
      "Epoch 15: loss=6.62758\n",
      "Epoch 16: loss=6.61216\n",
      "Epoch 17: loss=6.62161\n",
      "Epoch 18: loss=6.19454\n",
      "Epoch 19: loss=6.26049\n",
      "Epoch 20: loss=6.28317\n",
      "Epoch 21: loss=6.06780\n",
      "Epoch 22: loss=5.85774\n",
      "Epoch 23: loss=5.68926\n",
      "Epoch 24: loss=5.57079\n",
      "Epoch 25: loss=5.17103\n",
      "Epoch 26: loss=5.26013\n",
      "Epoch 27: loss=5.22982\n",
      "Epoch 28: loss=5.11142\n",
      "Epoch 29: loss=5.01365\n",
      "Epoch 30: loss=4.69790\n",
      "Epoch 31: loss=4.83433\n",
      "Epoch 32: loss=4.67190\n",
      "Epoch 33: loss=4.63691\n",
      "Epoch 34: loss=4.52287\n",
      "Epoch 35: loss=4.61028\n",
      "Epoch 36: loss=4.44076\n",
      "Epoch 37: loss=4.41658\n",
      "Epoch 38: loss=4.46938\n",
      "Epoch 39: loss=4.51879\n",
      "Epoch 40: loss=4.44682\n",
      "Epoch 41: loss=4.36723\n",
      "Epoch 42: loss=4.36634\n",
      "Epoch 43: loss=4.47352\n",
      "Epoch 44: loss=4.40003\n",
      "Epoch 45: loss=4.34512\n",
      "Epoch 46: loss=4.42904\n",
      "Epoch 47: loss=4.32510\n",
      "Epoch 48: loss=4.29971\n",
      "Epoch 49: loss=4.38928\n",
      "Epoch 50: loss=4.32877\n",
      "Epoch 51: loss=4.30054\n",
      "Epoch 52: loss=4.34691\n",
      "Epoch 53: loss=4.42032\n",
      "Epoch 54: loss=4.36494\n",
      "Epoch 55: loss=4.34286\n",
      "Epoch 56: loss=4.39150\n",
      "Epoch 57: loss=4.39426\n",
      "Epoch 58: loss=4.34069\n",
      "Epoch 59: loss=4.30878\n",
      "Epoch 60: loss=4.31257\n",
      "Epoch 61: loss=4.37032\n",
      "Epoch 62: loss=4.28523\n",
      "Epoch 63: loss=4.33260\n",
      "Epoch 64: loss=4.26903\n",
      "Epoch 65: loss=4.34236\n",
      "Epoch 66: loss=4.29934\n",
      "Epoch 67: loss=4.26983\n",
      "Epoch 68: loss=4.31322\n",
      "Epoch 69: loss=4.32200\n",
      "Epoch 70: loss=4.27024\n",
      "Epoch 71: loss=4.30791\n",
      "Epoch 72: loss=4.29230\n",
      "Epoch 73: loss=4.26509\n",
      "Epoch 74: loss=4.25552\n",
      "Epoch 75: loss=4.28404\n",
      "Epoch 76: loss=4.23568\n",
      "Epoch 77: loss=4.37057\n",
      "Epoch 78: loss=4.27412\n",
      "Epoch 79: loss=4.25780\n",
      "Epoch 80: loss=4.25622\n",
      "Epoch 81: loss=4.28220\n",
      "Epoch 82: loss=4.28520\n",
      "Epoch 83: loss=4.21614\n",
      "Epoch 84: loss=4.19727\n",
      "Epoch 85: loss=4.27099\n",
      "Epoch 86: loss=4.22037\n",
      "Epoch 87: loss=4.26535\n",
      "Epoch 88: loss=4.22575\n",
      "Epoch 89: loss=4.25997\n",
      "Epoch 90: loss=4.18363\n",
      "Epoch 91: loss=4.18884\n",
      "Epoch 92: loss=4.20659\n",
      "Epoch 93: loss=4.17392\n",
      "Epoch 94: loss=4.18927\n",
      "Epoch 95: loss=4.22815\n",
      "Epoch 96: loss=4.20759\n",
      "Epoch 97: loss=4.24895\n",
      "Epoch 98: loss=4.17411\n",
      "Epoch 99: loss=4.18847\n"
     ]
    }
   ],
   "source": [
    "training_loop(vit, train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련후 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LinearProjection at 0x7f056fdf75b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.submodules[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 제너레이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCSVData(path, rootpath):\n",
    "\n",
    "    filecsv   = open(path)\n",
    "    csvreader = csv.reader(filecsv)\n",
    "    rows = []\n",
    "    \n",
    "    header = []\n",
    "    header = next(csvreader)\n",
    "    \n",
    "    for row in csvreader:\n",
    "        try:\n",
    "\n",
    "            impath =  \"{}/{}jpg\".format(rootpath, row[0])\n",
    "        \n",
    "            new_row = {'elevation':float(row[1]),\n",
    "                    'azimuth': float(row[2]),\n",
    "                    'impath':impath}\n",
    "            \n",
    "            rows.append(new_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rows: 28796\n",
      "Found 18448 validated image filenames.\n",
      "Found 4612 validated image filenames.\n",
      "/home/cvnar1/Desktop/teamHDR/data\n",
      "Found 4043 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvnar1/anaconda3/envs/tensorflow2.8/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 5736 invalid image filename(s) in x_col=\"impath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/home/cvnar1/anaconda3/envs/tensorflow2.8/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 5736 invalid image filename(s) in x_col=\"impath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/home/cvnar1/anaconda3/envs/tensorflow2.8/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 3155 invalid image filename(s) in x_col=\"impath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_csvpath = os.path.join(root_data,\"train_listed.csv\")\n",
    "train_datapath = os.path.join(root_data,\"ldr_train/\")\n",
    "train_rows = loadCSVData(train_csvpath,train_datapath)\n",
    "print(\"train_rows:\",len(train_rows))\n",
    "\n",
    "train_df = pd.DataFrame.from_records(train_rows)\n",
    "train_df = train_df.sample(frac=1.0,  random_state=1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "train_images  = train_generator.flow_from_dataframe(\n",
    "    dataframe =  train_df,\n",
    "    x_col = 'impath',\n",
    "    y_col = ['elevation', 'azimuth'],\n",
    "    target_size = input_shape[:2],\n",
    "    #target_size = (120,120),\n",
    "    #color_mode='grayscale',\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'raw',\n",
    "    batch_size =BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    seed=SEED,\n",
    "    subset='training'\n",
    ")\n",
    "val_images  = train_generator.flow_from_dataframe(\n",
    "    dataframe =  train_df,\n",
    "    x_col = 'impath',\n",
    "    y_col = ['elevation', 'azimuth'],\n",
    "    target_size = input_shape[:2],\n",
    "    #target_size = (120,120),\n",
    "    #color_mode='grayscale',\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'raw',\n",
    "    batch_size =BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    seed=SEED,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "print(root_data)\n",
    "test_csvpath  =  os.path.join(root_data,\"val_listed.csv\")\n",
    "test_datapath =  os.path.join(root_data,\"ldr_val/\")\n",
    "test_rows = loadCSVData(test_csvpath, test_datapath)\n",
    "\n",
    "test_df = pd.DataFrame.from_records(test_rows)\n",
    "# test_df = test_df.sample(frac=1.0,  random_state=1)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "test_images  = test_generator.flow_from_dataframe(\n",
    "    dataframe =  test_df,\n",
    "    x_col = 'impath',\n",
    "    y_col = ['elevation', 'azimuth'],\n",
    "    target_size = (256,1024),\n",
    "    #target_size = (500,500),\n",
    "    #target_size = (120,120),\n",
    "    #color_mode='grayscale',\n",
    "    color_mode='rgb',\n",
    "    class_mode = 'raw',\n",
    "    batch_size =BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "test_labels = test_images.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"encoder_block_layer_norm\" (type LayerNormalization).\n\nInput to reshape is a tensor with 10 values, but the requested shape has 10000 [Op:Reshape]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(10000, 5, 20), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb 셀 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_loop(vit, x_test, y_test)\n",
      "\u001b[1;32m/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb 셀 37\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_loop\u001b[39m(model, x, y):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Update the model with the single giant batch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train(model, x, y, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Track this before I update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Ws.append(model.w.numpy())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# bs.append(model.b.numpy())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     current_loss \u001b[39m=\u001b[39m cce(y, model(x))\n",
      "\u001b[1;32m/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb 셀 37\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y, learning_rate)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, x, y, learning_rate):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Trainable variables are automatically tracked by GradientTape\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     current_loss \u001b[39m=\u001b[39m cce(y, model(x))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39m# Use GradientTape to calculate the gradients with respect to W and b\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   gradients \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mgradient(current_loss, model\u001b[39m.\u001b[39mtrainable_variables)\n",
      "\u001b[1;32m/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb 셀 37\u001b[0m in \u001b[0;36mVisionTransformer.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# (batch, height, width)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     patches \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mextract_patches(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         sizes\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size,\u001b[39m1\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVALID\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(patches)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_token \u001b[39m=\u001b[39m encoded[:,\u001b[39m0\u001b[39m,:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_token\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "\u001b[1;32m/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb 셀 37\u001b[0m in \u001b[0;36mEncoder.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embedding(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_num):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_block(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# self.encoded_norm = self.encoder_norm(self.encoded)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mTODO check encoder norm is needed.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mencoded_norm makes the final result all zero.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "\u001b[1;32m/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb 셀 37\u001b[0m in \u001b[0;36mEncoderBlock.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMHA(x1,x1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar1/Desktop/model/ViT/ViT_tfmodule.ipynb#Y132sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x1)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2.8/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2.8/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"encoder_block_layer_norm\" (type LayerNormalization).\n\nInput to reshape is a tensor with 10 values, but the requested shape has 10000 [Op:Reshape]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(10000, 5, 20), dtype=float32)"
     ]
    }
   ],
   "source": [
    "training_loop(vit, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow2.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05a0b0d24b486f762a660818e658a0e3bfd75854f8c75d6662c85db23d29aee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
